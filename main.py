import os
from dotenv import load_dotenv
from langchain import hub
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from openai import responses

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# query = 'What is pinecone in machine learning? í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”'
query = 'ì´ì¬ëª…ì˜ ìŠ¬ë¡œê±´ê³¼ ì •ì¹˜ì  ë°°ê²½ ë“± ì•„ëŠ”ëŒ€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”'


def basic_rag():
    # 1. ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”(index name, embeddigs)
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
    # vectorstore = PineconeVectorStore(embedding=embeddings, index_name=os.getenv('INDEX_NAME'))
    vectorstore = PineconeVectorStore(embedding=embeddings, index_name=os.getenv('INDEX_NAME_PRESIDENT'))

    # 2. langchain-ai/retrieval-qa-chat í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt = hub.pull('langchain-ai/retrieval-qa-chat')  # input_variables=['context', 'input']
    print(f'prompt: \n {prompt}')

    # 3. ì²´ì¸ ì„¤ì •
    # create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)
    # 3-1. ì²´ì¸ ìƒì„± (ì´ˆê¸°í™”)
    llm = ChatOpenAI(model='gpt-4o-mini')

    # ìœ ì‚¬ë„ ê²€ìƒ‰
    search_docs = vectorstore.similarity_search(query=query, k=3)
    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹  ê°ì²´ ì •ì˜
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    # 3-2. ì²´ì¸ ì‹¤í–‰ (ì´ë•Œ ë¬¸ì„œ í•©ì³ì§)
    # â­ï¸ë°©ë²• 1
    response_stuff = combine_docs_chain.invoke({
        "context": search_docs, # ì´ë•Œ ì—¬ëŸ¬ê°œì˜ ë¬¸ì„œ í•¨ì³ì§
        "input": query
    })

    print(f'ğŸ”¥ğŸ”¥ìµœì¢…ë‹µë³€ combine_docs_chain: {response_stuff}') # â­â­ë¬¸ì„œì— fití•œ ë‹µë³€ - str

    # â­ï¸ë°©ë²• 2
    # 4. ì™„ì „í•œ ragì²´ì¸ ìƒì„± ë° ì§ˆë¬¸

    # ê²€ìƒ‰ê¸° ì¤€ë¹„
    retrieval = vectorstore.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever=retrieval, combine_docs_chain=combine_docs_chain)
    response_retrieval = retrieval_chain.invoke({
        'input': query # ì§ˆë¬¸ë§Œ ë„˜ê¸°ë©´ ë
    })

    print(f'ğŸ”¥ğŸ”¥ìµœì¢…ë‹µë³€ retrieval_chain: {response_retrieval.get("answer")}')  # â­â­ë¬¸ì„œì— fití•œ ë‹µë³€ - dict(input, context, answer)



def lcel_rag():
    print('...')
    # ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    # template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•´ì„œ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— í•œê¸€ë¡œ ë‹µí•˜ì„¸ìš”.
    # ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , ë‹µì„ ì§€ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ˆì„¸ìš”.
    # ìµœëŒ€ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ê³  ê°€ëŠ¥í•œ í•œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
    # ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ "ì§ˆë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!"ë¼ê³  ë§í•˜ì„¸ìš”.
    #
    # {context}
    #
    # ì§ˆë¬¸: {question}
    #
    # ë„ì›€ì´ ë˜ëŠ” ë‹µë³€:(í•œê¸€ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€)"""

def search_without_rag():
    prompt = PromptTemplate.from_template(query)
    llm = ChatOpenAI(model='gpt-4o-mini')
    chain = prompt | llm
    response = chain.invoke(input={})
    print(f'ğŸ”¥ğŸ”¥ìµœì¢… ë‹µë³€(ragì—†ì´): {response}') # â­ì¼ë°˜ì  ë‹µë³€



if __name__ == '__main__':
    # print('ragì—†ì´ ê¸°ë³¸ ì§ˆë¬¸í•˜ê¸°..')
    # search_without_rag()

    print('basic_rag ê²€ìƒ‰í•˜ê¸°..')
    basic_rag()

    print('LCELë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸°..')
    # basic_rag()


