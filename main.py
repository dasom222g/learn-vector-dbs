import os
from dotenv import load_dotenv
from langchain import hub
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def basic_rag():
    # 1. ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”(index name, embeddigs)
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
    vetorstor = PineconeVectorStore(embedding=embeddings, index_name=os.getenv('INDEX_NAME'))

    # 2. langchain-ai/retrieval-qa-chat í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt = hub.pull('langchain-ai/retrieval-qa-chat')  # input_variables=['context', 'input']
    print(f'prompt: \n {prompt}')

    # 3. ì²´ì¸ ì„¤ì •
    # create_stuff_documents_chain: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ë¡œ í•©ì³ì„œ LLMì—ê²Œ ì „ë‹¬í•˜ëŠ” ì²´ì¸ì„ ë§Œë“œëŠ” í•¨ìˆ˜
    # create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)
    # 3-1. ì²´ì¸ ìƒì„± (ì´ˆê¸°í™”)
    llm = ChatOpenAI(model='gpt-4o-mini')

    # ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ì—¬ê¸°ë¶€í„°
    # combined_docs = create_stuff_documents_chain(llm=llm)

    # 3-2. ì²´ì¸ ì‹¤í–‰ (ì´ë•Œ ë¬¸ì„œ í•©ì³ì§)

    # 4. ì™„ì „í•œ ragì²´ì¸ ìƒì„± ë° ì§ˆë¬¸

    # ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    # template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•´ì„œ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— í•œê¸€ë¡œ ë‹µí•˜ì„¸ìš”.
    # ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , ë‹µì„ ì§€ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ˆì„¸ìš”.
    # ìµœëŒ€ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ê³  ê°€ëŠ¥í•œ í•œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
    # ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ "ì§ˆë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!"ë¼ê³  ë§í•˜ì„¸ìš”.
    #
    # {context}
    #
    # ì§ˆë¬¸: {question}
    #
    # ë„ì›€ì´ ë˜ëŠ” ë‹µë³€:(í•œê¸€ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€)"""


if __name__ == '__main__':
    print('basic_rag ê²€ìƒ‰í•˜ê¸°..')
    basic_rag()

    print('LCELë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸°..')
    # basic_rag()


